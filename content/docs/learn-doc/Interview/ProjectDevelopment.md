---
title: 项目经历
authors:
  - EndlessPeak
toc: true
featuredImage: 
date: 2022-03-04
hidden: false
draft: false
weight: 9
---

你本科阶段有哪些项目经历？

做过三个项目，分别是一个使用Java Server Page的考务管理系统，一个使用MybatisPlus + Springboot的商城系统，还有一个是有关三种传统机器学习算法的实现，使用的是最大信息增益比率的决策树、序列最小最优化算法的支持向量机和前馈神经网络。

## Java Web

### 综合问题

1. 使用的是什么技术，采用的是什么架构？

   考务管理系统使用的是JSP+JavaBean+Servlet；

   商城项目使用的是MybatisPlus + Spring Boot；

   采用的是MVC模式，分别是：

   1. M for Model，代表业务模型
   2. V for view，代表视图层
   3. C for controller，代表控制器

   对于具体项目的对应关系，

   1. JSP项目中，JavaBean是业务模型M，JSP是视图层V，Servlet是控制器C；
   2. SpringBoot项目中，JSP→Web/Controller→Service→ServiceImpl→Dao/Mapper→Bean
            1. Bean是各个基础类，属于业务模型；
            2. Dao提供基础操作，调用数据库；
            3. Service提供相应的服务，分模块调用各个Dao；
            4. ServiceImpl是它们的具体实现；
            5. Controller实现相应的功能，分模块请求多个服务，还包含请求服务时的预处理；
            6. JSP是MVC的View层，负责展示具体的页面；

2. 服务器中404 502 503 504错误分别代表什么？

### Java Servlet

1. Servlet中提供了两种请求方法，doGet和doPost方法，它们的联系和区别为何？

   它们都是HTTP协议中的两种请求发送的方法，都是TCP连接。

   1. get把参数包含在URL中，post通过request body传递参数；

   2. get请求只能url编码，post支持多种编码方式；

   3. get请求有长度限制，post请求则无此限制；

   4. get仅接受ASCII字符，post请求无此限制；

   5. get产生一个TCP数据包，post产生两个TCP数据包；

      get在传输层逻辑上只传输一次，服务器返回200 ok；

      post逻辑上传输两次，先传header，服务器返回100 contiune，再传data，服务器返回200 ok；

2. JSP中使用的代码复用技术

   在查询/删除时，结果可能存在一个或多个，一开始设计的是判断操作数量，然后分别调用功能函数。重构时，上层不关心具体的实现细节，也无需判断可能的操作数量。

   对于查询/删除函数，查询时统一使用Bean类型的对象的数组存储结果并返回，SQL语句仅一条，使用ResultSet提供的方法(**待完善**)进行遍历，避免在函数编写和调用时的复杂性；而删除时（**待完善**）

3. 前端接收数据格式

   前端Layui使用json字符串格式数据渲染表格，因此需要将object对象转化为json格式。使用的是fastjson包。

   定义转换函数：

   1. 将数据object附加上code，message，count等信息；
   2. 初始化jsonObject对象，使用jsonObject.put()方法依次加入code，message，count信息；
   3. 使用json.toJSONString(object)将object转换为JSONObject；
   4. 由于object可能有多个，将JSONObject装入JSONArray后再加入JSONObject；
   5. 最后将JSONObject转为JSONString格式；

4. XXX

### Spring Boot

1. Springboot中使用的代码复用技术

2. 什么是Map，为什么用Map

   Map是一个接口，它的每个元素包含一个key对象和一个value对象，key必须是唯一的且不能重复。

   Map的实现类包括HashMap、TreeMap、HashTable。

   1. TreeMap是有序的，HashMap和HashTable是无序的；
   2. Hashtable的方法是同步的（线程安全），HashMap的方法是异步的（非线程安全）；
   3. Hashtable不允许null值，HashMap允许null值（key和value都允许）

   Map的通用方法有：

   1. 返回map集合大小 `int size;`

   2. 判断map集合为空 `boolean isEmpty();`

   3. 根据key值获取value `Object get(Object key);`

   4. 添加元素\<key value> `Object put(Object key,Object value);`

   5. 获取元素的键和值的接口；

      ```java
       interface Entry<k,v>{
       	k getKey();
       	v getValue();
       }
      ```

3. 注解@Override

   1. 作为注释使用；
   2. 代码规范，对父类方法的重写；
   3. 编译器验证父类中是否有此方法；

## 机器学习

### 综合问题

1. 入侵检测技术的原理如何？

   包括基于数据源划分和基于检测技术的划分。数据源就是数据如何产生，可基于主机或基于网络；检测技术则分为基于误用和基于异常。基于误用可以理解为模式匹配，基于异常是只要不合法就判断为非法。入侵检测技术可简化成一种二分类问题。

2. 如何选择研究方法？

   由于使用基于异常的检测技术，也就是对正常或者不正常的访问进行划分，数据集中包含了最终的分类标签，因此采用的是监督学习。

   在生成方法和判别方法中，生成方法是统计得到联合概率，计算后得到条件概率；判别方法则直接学习条件概率，因此判别方法可以**简化学习问题**。

   毕业课题中，第一，所有的变量理论上都可以观测到值，不存在隐变量；第二，实际应用中部署的是训练好的模型，生成模型学习收敛速度快这点并不占优势，最后是由于判别方法直接学习条件概率或决策函数，准确率较高。

3. 什么是损失函数，什么是经验风险，什么是期望风险，什么是结构风险？

   1. 损失函数：针对单个具体样本，表示模型预测值与真实样本值之间的差距。常见的损失函数有0-1损失函数、平方损失函数、绝对损失函数、对数损失函数（对数似然损失函数）。
   2. 经验风险：对所有训练样本的**损失函数的平均值**，或模型对**所有训练样本**的预测能力。
   3. 期望风险：对所有样本损失函数的平均值，或模型对未知样本和已知的训练样本的综合预测能力。
   4. 结构风险：是对经验风险和期望风险的折中，在经验风险函数后面加一个正则化项。

4. 为什么使用决策树、支持向量机、前馈神经网络算法实现入侵检测？

   选择决策树是因为它原理简单，可解释性高，可视性好，一开始使用迭代二分法决策树作为原型，后面改用最大信息增益比率决策树进行实现，原因是需要对连续值进行划分；选择支持向量机则是因为它是传统机器学习算法中表现最优异的一种，推导严谨，分类比较精确，具体实现上使用序列最小优化算法进行实现，后面借鉴了一下最小二乘法的实现；选择前馈神经网络，它结构简单，实现快，可用它与传统机器学习算法进行对照。

### 数据处理

1. 你的数据集是怎样选取的，为什么要这样选择？

   我选择的数据集是CIC-IDS-2017，基于以下原因。（**待完善**）

2. 你对数据做了哪些预处理？有哪些改进的方法？

### 决策树

1. 简要描述一下决策树的原理
   1. 决策树解决二分类问题时，每一次决策都选择一种最佳属性进行划分，最终得到结果。
   2. 为了得到最佳划分，就需要在每次划分中获得更多的信息，引入**信息熵**（信息的混乱程度）来刻画划分过程中获得的信息量大小；
   3. 信息熵根据信息整体的随机变量进行定义，度量的是整体信息的混乱程度，因此当需要对某种属性进行划分时就需要**条件熵**；
   4. 根据在数据集整体上分类的不确定性（信息熵）减去在数据集上根据属性A分类的不确定性（条件熵）称为**信息增益**。信息增益越大，分类能力就越强。
   5. 使用信息增益，分类器会倾向于选择每个类别更少的样本的分类方式（比如说样本编号、网络端口），为了消除这种影响，将信息增益除以原来的熵，使划分结果更加科学。每步中都计算每个特征的信息增益比，选取信息增益比最大的特征作为最佳划分特征。
   6. 连续值离散化：连续型属性的值从小到大进行排序，计算各区间的中位点，组成候选划分点，依次计算以这些划分点划分的信息增益，当信息增益最大时选择此划分点作为该属性的最佳划分点

### 支持向量机

1. 支持向量机使用的是什么类型的间隔？为什么？

   使用几何间隔定义。因为函数间隔只需要成比例的改变w和b，函数间隔就会发生变化，而实际分类超平面并未变化。对超平面的w和b加上规范化约束，如除以w的L2范数，可以使得间隔是确定的。最后通过几何间隔和函数间隔之间的关系转化最优化问题。
   
2. 简要描述一下支持向量机的原理
   1. 支持向量机学习的结果是在高维空间中划分出一条分类超平面（同时也是判别函数），得到一个约束最优化问题，求解以得到分类超平面的各个参数；
   2. 通过非线性变换将问题转化为线性问题，使用的是高斯核函数/径向基核函数；
   3. 求解参数使用序列最小优化算法；
   4. 其中，线性分类中y=wx+b，w是行向量，x是列向量，b是一个数；
   5. 支持向量机的损失函数是：合页损失函数；
   6. 支持向量机有三宝，间隔（几何间隔转为函数间隔），对偶（拉格朗日对偶），核技巧。
3. 简要描述一下序列最小优化算法：
   1. 序列最小优化算法是一种启发式算法。如果所有的变量都满足最优化问题的KKT条件，那么最优化问题的解就得到了。（**KKT条件是最优化问题的充要条件**）
   2. 先固定N-2个变量，迭代2个变量，重复直到收敛；外层选择违反KKT条件程度最大的点，内层循环使另一个变量产生足够大的差异（具体实现上是选择差值最大$|E_2-E_1|$的变量；选择边界变量；随机选择变量），最后更新阈值和差值

### 前馈神经网络

1. 简要描述一下前馈神经网络算法的实现原理
   1. 确定每层隐藏层的层数、激活函数，根据净输入计算输出和活性值，然后传递给下一层，直到最终的输出层；
   2. 通过输出层的内容与分类结果比对，记录误差；
   3. 通过后一层的误差计算前一层的误差；计算该层的权重梯度和偏置梯度，然后更新参数，直到最前一层隐藏层。

2. 什么是激活函数？为什么需要激活函数？有哪些激活函数？

   1. 神经网络中每个神经节点接受上一层的输出作为本层的输入，并将输出传给下一层。上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。
   2. 激活函数的性质：连续并可导或少数点不可导的非线性函数，激活函数及其导数要尽可能简单；激活函数的导函数的值域要在一个合适的区间内；
   3. 激活函数的作用相当于支持向量机中的核技巧，使用非线性函数激活函数可以令神经网络具有逼近任意函数的能力，而不仅仅只是输入的线性组合。
   4. 激活函数例如Sigmoid型（包括Logistic函数和Tanh函数）又如ReLU型。

3. 什么是通用近似定理？

   神经网络的隐藏层在满足一定条件时，可以以任意精度近似任何一个在实数空间内的有界闭集函数。条件如下：

   1. 具有线性输出层和至少一个使用“挤压”性质（把无穷区间映射到有穷区间）的激活函数；
   2. 隐藏层内神经元数量足够多；

4. 神经网络中的参数的迭代方式是什么？

   是随机梯度下降迭代。具体过程如下：

   1. 参数更新：

      新参数等于旧参数减去学习率乘以偏导数（偏置参数）或偏导数加上$\lambda W^{l}$（权重参数）；
      $$
      W^l_{new}=W^l_{old}-\alpha(\frac{\partial \mathcal{L}(y^{(n)},\hat{y}^{(n)}}{\partial W^{l}}+\lambda W^{l})
      $$

      $$
      b^l_{new}=b^l_{old}-\alpha(\frac{\partial \mathcal{L}(y^{(n)},\hat{y}^{(n)}}{\partial b^{l}})
      $$

      

   2. 求误差：根据链式法则计算三个偏导数，先计算对每个元素的，然后扩展到矩阵；

      1. 第l层的净输入对第l层的权重向量的偏导（第i个元素为$a_j^{l-1}$，其余为0的行向量）；
      2. 第l层的净输入对第l层的偏置的偏导（单位矩阵）；
      3. 第l层的损失函数对第l层的净输入的偏导（表示第l层神经元对最终损失函数的影响，也称其为误差项）；

1. 什么是随机梯度下降？为什么要随机梯度下降迭代？
   1. 为了使结构化风险函数最小，需要优化其中的参数。
   2. 每次采集一个样本，计算这个样本的结构化风险函数的梯度并沿负方向更新参数。
   3. 其中结构化风险是经验风险加上一个参数的正则化项；经验风险是损失函数的平均值。
   4. 批量梯度下降的开销太大，每次只计算一个样本可以简化计算，通过梯度下降找到局部最优，通过随机噪声跳出局部最优解。降低开销，提高收敛速度。

### 具体实现

问：Python的四大数据类型是哪些？

答：元组、列表、字典、字符串

### 改进方向

决策树：CART树

支持向量机：最小二乘法、加权、稀疏核机

神经网络：循环神经网络

