---
title: 前馈神经网络问题
authors:
  - EndlessPeak
toc: true
featuredImage: 
date: 2022-03-25
hidden: false
draft: false
weight: 4
---

## 原理

1. 简要描述一下前馈神经网络算法的实现原理

   1. 确定每层隐藏层的层数、激活函数，根据净输入计算输出和活性值，然后传递给下一层，直到最终的输出层；
   2. 通过输出层的内容与分类结果比对，记录误差；
   3. 通过后一层的误差计算前一层的误差；计算该层的权重梯度和偏置梯度，然后更新参数，直到最前一层隐藏层。

2. 什么是激活函数？为什么需要激活函数？有哪些激活函数？

   1. 神经网络中每个神经节点接受上一层的输出作为本层的输入，并将输出传给下一层。上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。
   2. 激活函数的性质：连续并可导或少数点不可导的非线性函数，激活函数及其导数要尽可能简单；激活函数的导函数的值域要在一个合适的区间内；
   3. 激活函数的作用相当于支持向量机中的核技巧，使用非线性函数激活函数可以令神经网络具有逼近任意函数的能力，而不仅仅只是输入的线性组合。
   4. 激活函数例如Sigmoid型（包括Logistic函数和Tanh函数）又如ReLU型。

3. 什么是通用近似定理？

   神经网络的隐藏层在满足一定条件时，可以以任意精度近似任何一个在实数空间内的有界闭集函数。

   条件如下：

   1. 具有线性输出层和至少一个使用“挤压”性质（把无穷区间映射到有穷区间）的激活函数；
   2. 隐藏层内神经元数量足够多；

4. 神经网络中的参数的迭代方式是什么？

   是随机梯度下降迭代。具体过程如下：

   1. 参数更新：

      新参数等于旧参数减去学习率乘以偏导数（偏置参数）或偏导数加上$\lambda W^{l}$（权重参数）；
      $$
      W^l_{new}=W^l_{old}-\alpha(\frac{\partial \mathcal{L}(y^{(n)},\hat{y}^{(n)}}{\partial W^{l}}+\lambda W^{l})
      $$

      $$
      b^l_{new}=b^l_{old}-\alpha(\frac{\partial \mathcal{L}(y^{(n)},\hat{y}^{(n)}}{\partial b^{l}})
      $$

      

   2. 求误差：根据链式法则计算三个偏导数，先计算对每个元素的，然后扩展到矩阵；

      1. 第l层的净输入对第l层的权重向量的偏导（第i个元素为$a_j^{l-1}$，其余为0的行向量）；
      2. 第l层的净输入对第l层的偏置的偏导（单位矩阵）；
      3. 第l层的损失函数对第l层的净输入的偏导（表示第l层神经元对最终损失函数的影响，也称其为误差项）；

5. 什么是随机梯度下降？为什么要随机梯度下降迭代？

   1. 为了使结构化风险函数最小，需要优化其中的参数。

   2. 每次采集一个样本，计算这个样本的结构化风险函数的梯度并沿负方向更新参数。

      沿负方向的目的是使结构化风险函数最小化。

   3. 其中结构化风险是经验风险加上一个参数的正则化项；经验风险是损失函数的平均值。

   4. 批量梯度下降的开销太大，每次只计算一个样本可以简化计算，通过梯度下降找到局部最优，通过随机噪声跳出局部最优解。降低开销，提高收敛速度。

##  实现思路
