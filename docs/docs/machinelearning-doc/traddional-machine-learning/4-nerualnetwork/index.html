<!DOCTYPE html>
<html
  lang="zh"
  dir="ltr"
  
><meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>前馈神经网络问题 | Machine Learning Doc | 悦翎轩</title>

<meta name="generator" content="Hugo Eureka 0.9.3" />
<link rel="stylesheet" href="https://endlesspeak.github.io/css/eureka.min.3f9c102b2958d0308ef084ee682440b37aa0ca93912ed5c0980028307f924cfb413f8214a8ae653b7def936d6a1bc390.css">
<script defer src="https://endlesspeak.github.io/js/eureka.min.fa9a6bf6d7a50bb635b4cca7d2ba5cf3dfb095ae3798773f1328f7950028b48c17d06276594e1b5f244a25a6c969a705.js"></script>

















<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link rel="preload"
  href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&amp;family=Noto&#43;Sans&#43;SC:wght@400;600;700&amp;family=Source&#43;Sans&#43;Pro:wght@400;600;700&amp;display=swap"
  as="style" onload="this.onload=null;this.rel='stylesheet'">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/styles/nnfx-light.min.css"
   media="print"
  onload="this.media='all';this.onload=null" crossorigin>
<script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/highlight.min.js"
   crossorigin></script>
  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/c.min.js"
     crossorigin></script>
  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/cpp.min.js"
     crossorigin></script>
  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/java.min.js"
     crossorigin></script>
  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/python.min.js"
     crossorigin></script>
  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/dart.min.js"
     crossorigin></script>
  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/cmake.min.js"
     crossorigin></script>
  <script defer src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.4.0/build/languages/rust.min.js"
     crossorigin></script>
<link rel="stylesheet" href="https://endlesspeak.github.io/css/highlightjs.min.2958991528e43eb6fc9b8c4f2b8e052f79c4010718e1d1e888a777620e9ee63021c2c57ec7417a3108019bb8c41943e6.css" media="print" onload="this.media='all';this.onload=null">


<script defer type="text/javascript" src="https://endlesspeak.github.io/js/fontawesome.min.15ca3da36d9676aa223d8be3b1f49c6d893c4e16cbfce12124e114e34b1f20e7f5eee796fba6948cfac4c6a9ae381fcd.js"></script>




<script defer src="https://cdn.jsdelivr.net/npm/mermaid@8.14.0/dist/mermaid.min.js" 
  integrity="sha384-atOyb0FxAgN9LyAc6PEf9BjgwLISyansgdH8/VXQH8p2o5vfrRgmGIJ2Sg22L0A0"  crossorigin></script>




<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<script>
  MathJax = {
    tex: {
      inlineMath: [["$", "$"],["\\(", "\\)"]],
    },
    displayMath: [
      ["$$", "$$"],
      ["\[\[", "\]\]"],
    ],
    svg: {
      fontCache: "global",
    },
  };
</script>
<link rel="icon" type="image/png" sizes="32x32" href="https://endlesspeak.github.io/images/avtar_huc2c209728b40c4588688df8c11b591ef_46235_32x32_fill_box_center_3.png">
<link rel="apple-touch-icon" sizes="180x180" href="https://endlesspeak.github.io/images/avtar_huc2c209728b40c4588688df8c11b591ef_46235_180x180_fill_box_center_3.png">

<meta name="description"
  content="原理 简要描述一下前馈神经网络算法的实现原理 确定每层隐藏层的层数、激活函数，根据净输入计算输出和活性值，然后传递给下一层，直到最终的输出层； 通过输出层的">
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [{
      "@type": "ListItem",
      "position": 1 ,
      "name":"Docs",
      "item":"https://endlesspeak.github.io/docs/"},{
      "@type": "ListItem",
      "position": 2 ,
      "name":"Docs",
      "item":"https://endlesspeak.github.io/docs/docs/"},{
      "@type": "ListItem",
      "position": 3 ,
      "name":"Machine Learning Doc",
      "item":"https://endlesspeak.github.io/docs/docs/machinelearning-doc/"},{
      "@type": "ListItem",
      "position": 4 ,
      "name":"传统机器学习系列",
      "item":"https://endlesspeak.github.io/docs/docs/machinelearning-doc/traddional-machine-learning/"},{
      "@type": "ListItem",
      "position": 5 ,
      "name":"前馈神经网络问题",
      "item":"https://endlesspeak.github.io/docs/docs/machinelearning-doc/traddional-machine-learning/4-nerualnetwork/"}]
}
</script>



<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Article",
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "https://endlesspeak.github.io/docs/docs/machinelearning-doc/traddional-machine-learning/4-nerualnetwork/"
    },
    "headline": "前馈神经网络问题 | Machine Learning Doc | 悦翎轩","datePublished": "2022-03-25T00:00:00+00:00",
    "dateModified": "2022-03-25T00:00:00+00:00",
    "wordCount":  2743 ,
    "author": {
        "@type": "Person",
        "name": ["EndlessPeak"]
    },
    "publisher": {
        "@type": "Person",
        "name": "EndlessPeak",
        "logo": {
            "@type": "ImageObject",
            "url": "https://endlesspeak.github.io/images/avtar.png"
        }
        },
    "description": "原理 简要描述一下前馈神经网络算法的实现原理 确定每层隐藏层的层数、激活函数，根据净输入计算输出和活性值，然后传递给下一层，直到最终的输出层； 通过输出层的"
}
</script><meta property="og:title" content="前馈神经网络问题 | Machine Learning Doc | 悦翎轩" />
<meta property="og:type" content="article" />


<meta property="og:image" content="https://endlesspeak.github.io/images/avtar.png">


<meta property="og:url" content="https://endlesspeak.github.io/docs/docs/machinelearning-doc/traddional-machine-learning/4-nerualnetwork/" />




<meta property="og:description" content="原理 简要描述一下前馈神经网络算法的实现原理 确定每层隐藏层的层数、激活函数，根据净输入计算输出和活性值，然后传递给下一层，直到最终的输出层； 通过输出层的" />




<meta property="og:locale" content="zh" />




<meta property="og:site_name" content="悦翎轩" />






<meta property="article:published_time" content="2022-03-25T00:00:00&#43;00:00" />


<meta property="article:modified_time" content="2022-03-25T00:00:00&#43;00:00" />



<meta property="article:section" content="docs" />





  <body class="flex min-h-screen flex-col">
    <header
      class="min-h-16 pl-scrollbar bg-secondary-bg fixed z-50 flex w-full items-center shadow-sm"
    >
      <div class="mx-auto w-full max-w-screen-xl"><script>
    let storageColorScheme = localStorage.getItem("lightDarkMode")
    if (((storageColorScheme == 'Auto' || storageColorScheme == null) && window.matchMedia("(prefers-color-scheme: dark)").matches) || storageColorScheme == "Dark") {
        document.getElementsByTagName('html')[0].classList.add('dark')
    }
</script>
<nav class="flex items-center justify-between flex-wrap px-4 py-4 md:py-0">
    <a href="/" class="me-6 text-primary-text text-xl font-bold">悦翎轩</a>
    <button id="navbar-btn" class="md:hidden flex items-center px-3 py-2" aria-label="Open Navbar">
        <i class="fas fa-bars"></i>
    </button>

    <div id="target"
        class="hidden block md:flex md:grow md:justify-between md:items-center w-full md:w-auto text-primary-text z-20">
        <div class="md:flex md:h-16 text-sm md:grow pb-4 md:pb-0 border-b md:border-b-0">
            <a href="/authors" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">关于</a>
            <a href="/posts/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">文章</a>
            <a href="/docs/docs/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  selected-menu-item  me-4">文档</a>
            <a href="/docs/build/" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">构建</a>
            <a href="/categories" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">分类</a>
            <a href="/tags" class="block mt-4 md:inline-block md:mt-0 md:h-(16-4px) md:leading-(16-4px) box-border md:border-t-2 md:border-b-2  border-transparent  me-4">标签</a>
        </div>

        <div class="flex">
            <div class="relative pt-4 md:pt-0">
                <div class="cursor-pointer hover:text-eureka" id="lightDarkMode">
                    <i class="fas fa-adjust"></i>
                </div>
                <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-30" id="is-open">
                </div>
                <div class="absolute flex flex-col start-0 md:start-auto end-auto md:end-0 hidden bg-secondary-bg w-48 rounded py-2 border border-tertiary-bg cursor-pointer z-40"
                    id='lightDarkOptions'>
                    <span class="px-4 py-1 hover:text-eureka" name="Light">浅色</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Dark">深色</span>
                    <span class="px-4 py-1 hover:text-eureka" name="Auto">自动</span>
                </div>
            </div>
        </div>
    </div>

    <div class="fixed hidden inset-0 opacity-0 h-full w-full cursor-default z-0" id="is-open-mobile">
    </div>

</nav>
<script>
    
    let element = document.getElementById('lightDarkMode')
    if (storageColorScheme == null || storageColorScheme == 'Auto') {
        document.addEventListener('DOMContentLoaded', () => {
            window.matchMedia("(prefers-color-scheme: dark)").addEventListener('change', switchDarkMode)
        })
    } else if (storageColorScheme == "Light") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'sun')
        element.firstElementChild.classList.add('fa-sun')
    } else if (storageColorScheme == "Dark") {
        element.firstElementChild.classList.remove('fa-adjust')
        element.firstElementChild.setAttribute("data-icon", 'moon')
        element.firstElementChild.classList.add('fa-moon')
    }

    document.addEventListener('DOMContentLoaded', () => {
        getcolorscheme();
        switchBurger();
    });
</script>
</div>
    </header>
    <main class="grow pt-16">
        <div class="pl-scrollbar">
          <div class="mx-auto w-full max-w-screen-2xl lg:px-4 xl:px-8">


<div class="lg:pt-12">
    <div class="flex flex-col md:flex-row bg-secondary-bg rounded">
        <div class="md:w-1/4 lg:w-1/5 border-e">
            <div class="sticky top-16 pt-6">
                















<div id="sidebar-title" class="md:hidden mx-4 px-2 pt-4 pb-2 md:border-b text-tertiary-text md:text-primary-text">
    <span class="font-semibold">目录</span>
    <i class='fas fa-caret-right ms-1'></i>
</div>

<div id="sidebar-toc"
    class="hidden md:block overflow-y-auto mx-6 md:mx-0 pe-6 pt-2 md:max-h-doc-sidebar bg-primary-bg md:bg-transparent">
    <div class="flex flex-wrap ms-4 -me-2 p-2 bg-secondary-bg md:bg-primary-bg rounded">
        <a class=" hover:text-eureka"
            href="https://endlesspeak.github.io/docs/docs/machinelearning-doc/">Machine Learning Doc</a>
        
        
        


    </div>
    
<ul class="ps-6">
    
    
    
    <li class="py-2">
        <div class=" pb-2 ">
            <a class=" hover:text-eureka"
                href="https://endlesspeak.github.io/docs/docs/machinelearning-doc/traddional-machine-learning/">传统机器学习系列</a>
        </div>
        
        
<ul class="ps-6">
    
    
    
    <li class="py-2">
        <div class="">
            <a class=" hover:text-eureka"
                href="https://endlesspeak.github.io/docs/docs/machinelearning-doc/traddional-machine-learning/1-summarize/">机器学习综述</a>
        </div>
        
    </li>
    
    
    
    
    <li class="py-2">
        <div class="">
            <a class=" hover:text-eureka"
                href="https://endlesspeak.github.io/docs/docs/machinelearning-doc/traddional-machine-learning/2-decisiontree/">决策树问题</a>
        </div>
        
    </li>
    
    
    
    
    <li class="py-2">
        <div class="">
            <a class=" hover:text-eureka"
                href="https://endlesspeak.github.io/docs/docs/machinelearning-doc/traddional-machine-learning/3-svm-derivation/">支持向量机推导</a>
        </div>
        
    </li>
    
    
    
    
    <li class="py-2">
        <div class="">
            <a class=" hover:text-eureka"
                href="https://endlesspeak.github.io/docs/docs/machinelearning-doc/traddional-machine-learning/3-svm/">支持向量机问题</a>
        </div>
        
    </li>
    
    
    
    
    <li class="py-2">
        <div class="">
            <a class=" text-eureka  hover:text-eureka"
                href="https://endlesspeak.github.io/docs/docs/machinelearning-doc/traddional-machine-learning/4-nerualnetwork/">前馈神经网络问题</a>
        </div>
        
    </li>
    
    
</ul>

        
    </li>
    
    
</ul>

</div>







            </div>

        </div>
        <div class="w-full md:w-3/4 lg:w-4/5 pb-8 pt-2 md:pt-8">
            <div class="flex">
                <div class="w-full lg:w-3/4 px-6">
                    <article class="prose">
  <h1 class="mb-4">前馈神经网络问题</h1>

  <div
  class="text-tertiary-text not-prose mt-2 flex flex-row flex-wrap items-center"
>
  <div class="me-6 my-2">
    <i class="fas fa-calendar me-1"></i>
    <span
      >2022-03-25</span
    >
  </div>
  <div class="me-6 my-2">
    <i class="fas fa-clock me-1"></i>
    <span>6分钟阅读时长</span>
  </div>

  

  
</div>


  
  

  <h2 id="原理">原理</h2>
<ol type="1">
<li><p>简要描述一下前馈神经网络算法的实现原理</p>
<ol type="1">
<li>确定每层隐藏层的层数、激活函数，根据净输入计算输出和活性值，然后传递给下一层，直到最终的输出层；</li>
<li>通过输出层的内容与分类结果比对，记录误差；</li>
<li>通过后一层的误差计算前一层的误差；计算该层的权重梯度和偏置梯度，然后更新参数，直到最前一层隐藏层。</li>
</ol></li>
<li><p>什么是激活函数？为什么需要激活函数？有哪些激活函数？</p>
<ol type="1">
<li><p>神经网络中每个神经节点接受上一层的输出作为本层的输入，并将输出传给下一层。上层节点的输出和下层节点的输入之间具有一个函数关系，这个函数称为激活函数（又称激励函数）。</p></li>
<li><p>激活函数的性质：连续并可导或少数点不可导的非线性函数，激活函数及其导数要尽可能简单；激活函数的导函数的值域要在一个合适的区间内；</p></li>
<li><p>激活函数的作用相当于支持向量机中的核技巧，使用非线性函数激活函数可以令神经网络具有逼近任意函数的能力，而不仅仅只是输入的线性组合。</p></li>
<li><p>激活函数例如Sigmoid型（包括Logistic函数和Tanh函数）又如ReLU型。</p>
<p><span class="math display">\[
\begin{align}
&amp;sigmoid(x)=\frac{1}{1+e^{-x}}\\
&amp;tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}\\
&amp;tanh(x)=2\sigma(2x)-1\\
&amp;ReLU(x)=max(0,x)=
\begin{cases}
  \ x,x&gt;0\\
  \ 0,x&lt;0
\end{cases}
\end{align}
\]</span></p></li>
<li><p><strong>激活函数的性质</strong></p>
<ol type="1">
<li><p>Logistic 函数是“挤压”函数，值域<span
class="math inline">\((0,1)\)</span></p>
<p>对一些输入产生兴奋，对另一些输入产生抑制</p></li>
<li><p>Tanh 函数，值域<span class="math inline">\((-1,1)\)</span></p>
<p>零中心化（关于原点对称）。</p>
<p>特别地，非零中心化会使后一层神经元的输入发生偏置偏移，并使梯度下降收敛速度变慢。</p></li>
<li><p>ReLU 函数，值域<span
class="math inline">\((0,+\infty)\)</span></p>
<p>非零中心化。另外在训练时比较容易“死亡“，即某一次更新后ReLU神经元对所有数据都是0，且在以后的训练过程中永远不能被激活。</p></li>
</ol></li>
</ol></li>
<li><p>什么是通用近似定理？</p>
<p>神经网络的隐藏层在满足一定条件时，可以以任意精度近似任何一个在实数空间内的有界闭集函数。</p>
<p>条件如下：</p>
<ol type="1">
<li>具有线性输出层和至少一个使用“挤压”性质（把无穷区间映射到有穷区间）的激活函数；</li>
<li>隐藏层内神经元数量足够多；</li>
</ol></li>
<li><p>神经网络的前馈传播如何进行？</p>
<p>设输入<span
class="math inline">\(\alpha_i=[\alpha_{i1},...,\alpha_{im}]^T\)</span>，第l层第i个神经元的输入是<span
class="math inline">\(m\times 1\)</span>的向量。</p>
<p>设第l层权重矩阵的行向量<span
class="math inline">\(W_i^{(l)}=[W_{i1}^{(l)},...,W_{im}^{(l)}]\)</span>，第l层的第i个神经元的权重向量是<span
class="math inline">\(1\times m\)</span>向量。</p>
<p>设第i层有n个神经元（n的数量可以自由设置）。</p>
<ol type="1">
<li><p>第l层的净输入<span class="math inline">\(z^l\)</span>： <span
class="math display">\[
z^{(l)}=W^{(l)}\alpha^{(l-1)}+b^{(l)}
\]</span> 其中<span class="math inline">\(W^{(l)}\)</span>是<span
class="math inline">\(n\times m\)</span>矩阵，<span
class="math inline">\(\alpha^{(l-1)}\)</span>是<span
class="math inline">\(m\times 1\)</span>向量。</p></li>
<li><p>第l层的净输出<span class="math inline">\(\alpha^l\)</span>：
<span class="math display">\[
\alpha^{(l)}=f_l(z^{(l)})
\]</span> 其中<span class="math inline">\(\alpha^{(l)}\)</span>是<span
class="math inline">\(n\times 1\)</span>向量。</p></li>
<li><p>特别地，样本向量x作为第0层的净输出；最后一层的净输出作为整个函数的输出。</p></li>
</ol></li>
<li><p>神经网络中的参数的学习方式是什么？</p>
<p>损失函数为交叉熵损失函数，<strong>样本</strong>的损失函数为： <span
class="math display">\[
\mathcal{L}(y,\hat{y})=-ylog\hat{y}
\]</span> 训练集在数据集上的结构化风险函数为： <span
class="math display">\[
R(W,b)=\frac{1}{N}\sum\limits_{n=1}^{N}\mathcal{L}(y^{(n)},\hat{y}^{(n)})+\frac{1}{2}\lambda\Vert
W\Vert_F^2
\]</span> 正则化项是Frobenius范数的平方，也就是所有参数平方的和。</p>
<p>参数更新采用梯度下降法。具体过程如下：</p>
<p>新参数等于旧参数减去学习率乘以偏导数（偏置参数）或偏导数加上<span
class="math inline">\(\lambda W^{l}\)</span>（权重参数）； <span
class="math display">\[
W^l_{new}=W^l_{old}-\alpha(\frac{\partial
\mathcal{L}(y^{(n)},\hat{y}^{(n)})}{\partial W^{l}}+\lambda W^{l})
\]</span></p>
<p><span class="math display">\[
b^l_{new}=b^l_{old}-\alpha(\frac{\partial
\mathcal{L}(y^{(n)},\hat{y}^{(n)})}{\partial b^{l}})
\]</span></p></li>
<li><p>反向传播算法</p>
<p>先计算损失函数对<strong>每个元素</strong>的偏导数，然后合并到矩阵；根据链式法则：
<span class="math display">\[
\frac{\partial\mathcal{L}(y,\hat{y})}{\partial\omega_{ij}^{(l)}}=\frac{\partial\mathcal{L}(y,\hat{y})}{\partial
z^{(l)}}\cdot\frac{\partial z^{(l)}}{\partial\omega_{ij}^{(l)}}
\]</span></p>
<p><span class="math display">\[
\frac{\partial\mathcal{L}(y,\hat{y})}{\partial
b^{(l)}}=\frac{\partial\mathcal{L}(y,\hat{y})}{\partial
z^{(l)}}\cdot\frac{\partial z^{(l)}}{\partial b^{(l)} }
\]</span></p>
<p>由上式知需要计算三个偏导数：</p>
<ol type="1">
<li><p>第l层的损失函数对第l层的净输入的偏导</p>
<p>表示第l层神经元对最终损失函数的影响，也称其为误差项；</p>
<p><span class="math display">\[
\begin{aligned}
\delta^{(l)}&amp;=\frac{\partial\mathcal{L}(y,\hat{y})}{\partial
z^{(l)}}\\
&amp;=\frac{\partial\alpha^{(l)}}{\partial z^{(l)}}\frac{\partial
z^{(l+1)}}{\partial
\alpha^{(l)}}\frac{\partial\mathcal{L}(y,\hat{y})}{\partial z^{(l+1)}}\\
&amp;=f&#39;_l(z^{(l)})\odot\left((W^{(l+1)})\delta^{(l+1)})\right)\\
\end{aligned}
\]</span></p>
<p><strong>第l层的神经元误差项</strong>：是所有与该神经元相连的第l+1层的神经元的误差项的权重和再乘以该神经元激活函数的梯度。</p></li>
<li><p>第l层的净输入对第l层的权重向量的偏导</p>
<p>矩阵微分采用<strong>分母布局</strong>，一个列向量关于标量的偏导数为行向量。</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial z^{(l)}}{\partial\omega_{ij}^{(l)}}&amp;=\left[
\frac{\partial z_1^{(l)}}{\partial\omega_{ij}^{(l)}},...,\frac{\partial
z_i^{(l)}}{\partial\omega_{ij}^{(l)}},...,\frac{\partial
z_{M_l}^{(l)}}{\partial\omega_{ij}^{(l)}},
\right]\\
&amp;=\left[
0,...,\frac{\partial
(W_i^{(l)}\alpha^{(l-1)}+b_i^{(l)})}{\partial\omega_{ij}^{(l)}},...,0
\right]
\end{aligned}
\]</span></p>
<p>设结果为第i个元素为<span
class="math inline">\(a_j^{(l-1)}\)</span>，其余为0的行向量； <span
class="math inline">\(W_i^{(l)}\)</span>为权重矩阵<span
class="math inline">\(W^{(l)}\)</span>的第i行，即第i个神经元的权重向量。</p></li>
<li><p>第l层的净输入对第l层的偏置的偏导（单位矩阵）；最终得到下面的公式：</p></li>
</ol>
<p><span class="math display">\[
\begin{align}
&amp; \frac{\partial \mathcal{L}(y^{(n)},\hat{y}^{(n)})}{\partial
W^{l}}=\delta^{(l)}(\alpha^{(l-1)})^T\\
&amp; \frac{\partial \mathcal{L}(y^{(n)},\hat{y}^{(n)})}{\partial
b^{l}}=\delta^{(l)}
\end{align}
\]</span></p>
<ol start="4" type="1">
<li>层与层之间参数更新的方式是矩阵乘法。</li>
</ol></li>
<li><p>softmax函数</p>
<p>假设我们有一个数组V，Vi表示V中的第i个元素，那么这个元素的Softmax值为
<span class="math display">\[
S_i=\frac{e^{V_i}}{\sum\limits_je^{V_j}}
\]</span> 该元素的softmax值就是该元素的指数与所有元素指数和的比值。</p>
<p>定义交叉熵损失函数： <span class="math display">\[
Loss=-\sum\limits_i t_ilny_i
\]</span> 其中<span class="math inline">\(t_i\)</span>表示真实值，<span
class="math inline">\(y_i\)</span>​表示求出的softmax值。其中目标类的<span
class="math inline">\(t_i=1\)</span>，其他均为0。</p>
<p>当预测到第i个时，可以认为<span
class="math inline">\(t_i=1\)</span>，损失函数变成： <span
class="math display">\[
Loss_i=-lny_i
\]</span> 定义选到<span class="math inline">\(y_i\)</span>的概率为 <span
class="math display">\[
P_{f_{y_i}}=\frac{e^{f_{y_i}}}{\sum\limits_je^{f_{y_i}}}
\]</span> 把数值映射到0-1之间，和为1，则有 <span class="math display">\[
f_{y_i}=\frac{e^{f_{y_i}}}{\sum\limits_je^{f_{y_j}}}=1-\frac{\sum\limits_{j
\neq i}e^{f_{y_j}}}{\sum\limits_je^{f_{y_j}}}
\]</span> 对损失函数求导 <span class="math display">\[
\begin{aligned}
\frac{\partial Loss_i}{\partial f_{y_j}}&amp;=\frac{\partial
(-lny_i)}{\partial f_{y_j}}\\
&amp;=\frac{\partial
(-ln\frac{e^{f_{y_i}}}{\sum\limits_je^{f_{y_j}}})}{\partial f_{y_j}}\\
&amp;=-\frac{1}{\frac{e^{f_{y_i}}}{\sum\limits_je^{f_{y_j}}}}\cdot
\frac{\partial \frac{e^{f_{y_i}}}{\sum\limits_je^{f_{y_j}}}}{\partial
f_{y_i}}\\
&amp;=-\frac{\sum\limits_je^{f_{y_j}}}{e^{y_i}}\cdot
\frac{\partial(1-\frac{\sum\limits_{j \neq
i}e^{f_{y_j}}}{\sum\limits_je^{f_{y_j}}})}{\partial f_{y_i}}\\
&amp;=-\frac{\sum\limits_je^{f_{y_j}}}{e^{y_i}}\cdot (-\sum\limits_{j
\neq i}e^{y_j})\cdot \frac{\partial(\frac{1}{\sum\limits_j
e^{y_j}})}{\partial f_{y_i}}\\
&amp;=\frac{\sum\limits_j e^{f_{y_j}}\cdot \sum\limits_{j \neq
i}e^{y_j}}{e^{y_i}}\cdot \frac{-e^{y_i}}{(\sum\limits_{j}e^{y_j})^2}\\
&amp;=-\frac{\sum\limits_{j \neq
i}e^{f_{y_j}}}{\sum\limits_je^{f_{y_j}}}\\
&amp;=-(1-\frac{e^{f_{y_i}}}{\sum\limits_je^{f_{y_j}}})\\
&amp;=P_{f_{y_i}}-1
\end{aligned}
\]</span></p></li>
<li><p>什么是随机梯度下降？为什么要随机梯度下降？</p>
<ol type="1">
<li><p>为了使结构化风险函数最小，需要优化其中的参数。</p></li>
<li><p>每次采集一个样本，计算这个样本的结构化风险函数的梯度并沿负方向更新参数。</p>
<p>沿负方向的目的是使结构化风险函数最小化。</p></li>
<li><p>其中结构化风险是经验风险加上一个参数的正则化项；</p>
<p>经验风险是对所有<strong>训练样本</strong>的损失函数的平均值。</p></li>
<li><p>批量梯度下降的开销太大，每次只计算一个样本可以简化计算，通过梯度下降找到局部最优或鞍点，通过随机噪声跳出局部最优。降低开销，提高收敛速度。</p></li>
</ol></li>
<li><p>随机梯度下降的反向传播算法实现</p>
<ol type="1">
<li>随机初始化权重矩阵、偏置</li>
<li>当模型的错误率还在下降时，循环
<ol type="1">
<li>对训练样本集中的样本随机重排序</li>
<li>对每个样本，循环
<ol type="1">
<li>选取样本</li>
<li>前馈计算</li>
<li>反向传播</li>
</ol></li>
</ol></li>
</ol></li>
</ol>
<h2 id="实现思路">实现思路</h2>
<ol type="1">
<li><p>sigmoid函数实现</p>
<p>计算公式为： <span class="math display">\[
sigmoid(x)=\frac{1}{2}tanh(\frac{x}{2})
\]</span> 实现代码为：</p>
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sigmoid(x):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">0.5</span><span class="op">*</span>(<span class="dv">1</span><span class="op">+</span>np.tanh(<span class="fl">0.5</span><span class="op">*</span>x))</span></code></pre></div>
<p>该函数实际未用，实际使用的是<code>numpy.tanh(x)</code>，因为它能传入矩阵。</p></li>
<li><p>numpy拆分数组和合并数组</p>
<ol type="1">
<li>合并数组<code>dataSet=np.concatenate((x_train, y_train), axis=1)</code></li>
<li>拆分数组<code>x_train,y_train=np.split(dataSet,(76,),axis=1)</code></li>
</ol></li>
<li><p>numpy排序</p>
<p><code>numpy.sort(a,axis,kind,order)</code></p></li>
<li><p>numpy产生随机权重矩阵</p>
<ol type="1">
<li><code>numpy.random.rand(76,50)</code></li>
<li><code>numpy.random.randn(76,50)/np.sqrt(76)</code></li>
</ol></li>
<li><p>正向传播</p>
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>z1 <span class="op">=</span> a0.dot(W1) <span class="op">+</span> b1</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>a1 <span class="op">=</span> np.tanh(z1)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>z2 <span class="op">=</span> a1.dot(W2) <span class="op">+</span> b2</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>a2 <span class="op">=</span> np.tanh(z2)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>z3 <span class="op">=</span> a2.dot(W3) <span class="op">+</span> b3</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>a3 <span class="op">=</span> np.tanh(z3)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>z4 <span class="op">=</span> a3.dot(W4) <span class="op">+</span> b4</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 最后一层使用softmax函数作为输出层激活函数</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>exp_scores<span class="op">=</span>np.exp(z4)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 归一化概率</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>probs<span class="op">=</span>exp_scores<span class="op">/</span>np.<span class="bu">sum</span>(exp_scores,axis<span class="op">=</span><span class="dv">1</span>,keepdims<span class="op">=</span><span class="va">True</span>)</span></code></pre></div></li>
<li><p>反向传播</p>
<p><span
class="math inline">\(\delta^{(l)}\)</span>的更新公式见上。输出层为softmax函数。
<span class="math display">\[
\delta^{(l)}_k=p_k-1
\]</span> （该公式推导见上面。）</p>
<div class="sourceCode" id="cb3"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>delta4 <span class="op">=</span> probs</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>delta4[<span class="bu">range</span>(num_examples), y] <span class="op">-=</span> <span class="dv">1</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>dW4 <span class="op">=</span> (a3.T).dot(delta4)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>db4 <span class="op">=</span> np.<span class="bu">sum</span>(delta4, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>delta3 <span class="op">=</span> delta4.dot(W4.T) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> np.power(a3, <span class="dv">2</span>))</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co"># tanh(x)的导数是1-tanh^2(x)</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>dW3 <span class="op">=</span> (a2.T).dot(delta3)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>db3 <span class="op">=</span> np.<span class="bu">sum</span>(delta3, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>delta2 <span class="op">=</span> delta3.dot(W3.T) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> np.power(a2, <span class="dv">2</span>))</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>dW2 <span class="op">=</span> (a1.T).dot(delta2)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>db2 <span class="op">=</span> np.<span class="bu">sum</span>(delta2, axis<span class="op">=</span><span class="dv">0</span>, keepdims<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>delta1 <span class="op">=</span> delta2.dot(W2.T) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> np.power(a1, <span class="dv">2</span>))</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>dW1 <span class="op">=</span> np.dot(a0.T, delta1)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>db1 <span class="op">=</span> np.<span class="bu">sum</span>(delta1, axis<span class="op">=</span><span class="dv">0</span>)</span></code></pre></div></li>
<li><p>预测</p>
<p>正向传播一次，用softmax激活函数作输出层的激活函数，并归一化。</p>
<p>使用<code>numpy.argmax</code>输出最大值索引（分类正确结果是0或1，两个概率哪个高选择哪个）</p></li>
</ol>

</article>

                    
                    
                    
                    <div class="py-2">
  
    <div class="my-8 flex flex-col items-center md:flex-row">
      <a href="https://endlesspeak.github.io/authors/endlesspeak/" class="md:me-4 text-primary-text h-24 w-24">
        
        
          <img
            src="https://endlesspeak.github.io/images/avtar.png"
            class="bg-primary-bg w-full rounded-full"
            alt="Avatar"
          />
        
      </a>
      <div class="mt-4 w-full md:mt-0 md:w-auto">
        <a
          href="https://endlesspeak.github.io/authors/endlesspeak/"
          class="mb-2 block border-b pb-1 text-lg font-bold"
        >
          <h3>Serene Feather Pavilion</h3>
        </a>
        <span class="block pb-2">瞽者无以与乎文章之观,聋者无以与乎钟鼓之声。岂唯形骸有聋盲哉?</span>
        
          
          
          
          
          <a href="mailto:endlesspeak@163.com" class="me-2">
            <i class="fas fa-envelope"></i>
          </a>
        
          
          
          
          
          <a href="https://gitee.com/endlesspeak" class="me-2">
            <i class="fab fa-git-square"></i>
          </a>
        
          
          
          
          
          <a href="https://github.com/endlesspeak" class="me-2">
            <i class="fab fa-github"></i>
          </a>
        
          
          
          
          
          <a href="https://space.bilibili.com/316071845" class="me-2">
            <i class="fas fa-tv"></i>
          </a>
        
      </div>
    </div>
  
</div>

                    

                    



                    
  <div
    class="-mx-2 mt-4 flex flex-col border-t px-2 pt-4 md:flex-row md:justify-between"
  >
    <div>
      
        <span class="text-primary-text block font-bold"
          >上一页</span
        >
        <a href="https://endlesspeak.github.io/docs/docs/machinelearning-doc/traddional-machine-learning/3-svm/" class="block">支持向量机问题</a>
      
    </div>
    <div class="mt-4 md:mt-0 md:text-right">
      
    </div>
  </div>


                    



                </div>
                
            </div>

        </div>


    </div>
</div>

<script>
    document.addEventListener('DOMContentLoaded', () => {
        
        hljs.highlightAll();
        changeSidebarHeight();
        switchDocToc();
    })
</script>









          </div>
        </div>
      
    </main>
    <footer class="pl-scrollbar">
      <div class="mx-auto w-full max-w-screen-2xl"><div class="text-center p-6 pin-b">
    <p class="text-sm text-tertiary-text">&copy; 2021 <a href="https://www.wangchucheng.com/">C. Wang</a> and <a href="https://www.ruiqima.com/">R. Ma</a>
 &middot;  Powered by the <a href="https://github.com/wangchucheng/hugo-eureka" class="hover:text-eureka">Eureka</a> theme for <a href="https://gohugo.io" class="hover:text-eureka">Hugo</a></p>
</div></div>
    </footer>
  </body>
</html>
